{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Install necessary Python updates as well as AutoML and analytic libraries (7 minutes)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade google-cloud-automl --quiet\n",
    "!pip install --upgrade google-cloud-logging --quiet\n",
    "\n",
    "from google.cloud import storage, automl_v1beta1 as automl\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random \n",
    "import string\n",
    "from google.cloud import logging\n",
    "import re\n",
    "\n",
    "print(f'Requirement set at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create necessary variables (approx. 1 minute)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM = (''.join([random.choice(string.ascii_letters + string.digits) for n in range(6)])).lower() # RANDOM ID GENERATED\n",
    "PROJECT_ID = 'auto-modeling-v402' # Project Identifier\n",
    "REGION = 'us-central1' # Bucket region, not to be changed\n",
    "RAW_SB = PROJECT_ID + '_raw_data' \n",
    "MODELS_SB = PROJECT_ID + '_models_bucket'\n",
    "GCS_URI = 'gs://' + MODELS_SB # This must be an existing Cloud Storage\n",
    "DISPLAY_NAME = 'bank_dataset' + '_' + RANDOM # Dataset name\n",
    "MODEL_DN = 'bank_model' + '_' + RANDOM # Model name\n",
    "TARGET_COLUMN = 'y' # Select a column to be the target\n",
    "TRAIN_BUDGET = 2000 # Training budget in milli_node_hour (between 1000 and 72000)\n",
    "gcs_input_uris = ['gs://auto-modeling-v402_raw_data/raw_bank.csv'] # Dataset in CSV format with requirements: https://cloud.google.com/automl-tables/docs/prepare\n",
    "MODEL_EXPORT = \"tf_saved_model\" # Model format for the export\n",
    "print(f'Varibles set at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}, double check following:')\n",
    "print(f'   Project \\x1b[31m{PROJECT_ID}\\x1b[0m must exist...')\n",
    "print(f'   Cloud Storage Bucket \\x1b[31m{GCS_URI}\\x1b[0m must exist...')\n",
    "print(f'   Cloud Storage Bucket \\x1b[31mgs://{RAW_SB}\\x1b[0m must exist...')\n",
    "print(f'   Training data \\x1b[31m{gcs_input_uris}\\x1b[0m must exist in a supported CSV format...')\n",
    "print(f'   AutoML Tables Dataset name \\x1b[31m{DISPLAY_NAME}\\x1b[0m must be unique...')\n",
    "print(f'   AutoML Tables Model name \\x1b[31m{MODEL_DN}\\x1b[0m must be unique...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create our class instance (approx. 1 minute)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_client = logging.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "tables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=RAW_SB)\n",
    "automl_client = automl.AutoMlClient()\n",
    "prediction_client = automl.PredictionServiceClient()\n",
    "tables_client = automl.TablesClient(project=PROJECT_ID, region=REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)\n",
    "\n",
    "print(f'Varibles set at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create datasets in AutoML (approx. 1 minute)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = False\n",
    "try:\n",
    "    dataset = tables_client.get_dataset(dataset_display_name=DISPLAY_NAME)\n",
    "    print(f'Dataset retrieved at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "except:\n",
    "    new_dataset = True\n",
    "    dataset = tables_client.create_dataset(DISPLAY_NAME)\n",
    "    print(f'Dataset created at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import data into AutoML datasets from GCS (approx. 5 minutes)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting dataset import ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "if new_dataset:\n",
    "    try:\n",
    "        import_data_operation = tables_client.import_data(dataset=dataset, gcs_input_uris=gcs_input_uris)\n",
    "        print('Dataset import operation: {}'.format(import_data_operation))\n",
    "        import_data_operation.result()\n",
    "        print(f'Dataset import ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "    except:\n",
    "        print(f'Dataset was already imported at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set target columns and update nullable columns</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting dataset configuration ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "try:\n",
    "    table = tables_client.set_target_column(dataset=dataset, column_spec_display_name=TARGET_COLUMN)\n",
    "\n",
    "    for col in tables_client.list_column_specs(PROJECT_ID,REGION,dataset.name):\n",
    "        if TARGET_COLUMN == col.display_name:\n",
    "            continue\n",
    "        tables_client.update_column_spec(PROJECT_ID,\n",
    "                                         REGION,\n",
    "                                         dataset.name,\n",
    "                                         column_spec_display_name=col.display_name,\n",
    "                                         type_code=col.data_type.type_code,\n",
    "                                         nullable=True)\n",
    "\n",
    "    print(f'Dataset configuration ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "except:\n",
    "    print(f'Dataset import not finished: try again in a few minutes, {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train the model (approx. 2 hours)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "model = None\n",
    "try:\n",
    "    model = tables_client.get_model(model_display_name=MODEL_DN)\n",
    "except:\n",
    "    response = tables_client.create_model(\n",
    "        MODEL_DN,\n",
    "        dataset=dataset,\n",
    "        train_budget_milli_node_hours=TRAIN_BUDGET,\n",
    "        exclude_column_spec_names=[TARGET_COLUMN]\n",
    "    )\n",
    "    print('Training model operation: {}'.format(response.operation))\n",
    "    model = response.result()\n",
    "    model2 = tables_client.get_model(model_display_name=MODEL_DN)\n",
    "    MODEL_ID = model2.name.split(\"/\")\n",
    "    MODEL_ID = MODEL_ID[len(MODEL_ID) - 1]\n",
    "    print('Training model identifier: ' + MODEL_ID)\n",
    "\n",
    "print(f'Model training ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate evaluation metrics for analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_id = automl_client.model_path(PROJECT_ID, REGION, MODEL_ID)\n",
    "for e in automl_client.list_model_evaluations(model_full_id, \"\"):\n",
    "    evaluation = e\n",
    "    break\n",
    "model_evaluation_id = evaluation.name.split(\"{}/modelEvaluations/\".format(MODEL_ID))[1].split(\"\\n\")[0]\n",
    "model_full_id = automl_client.model_evaluation_path(PROJECT_ID, REGION, MODEL_ID, model_evaluation_id)\n",
    "response = automl_client.get_model_evaluation(model_full_id)\n",
    "data = [['AUC PR', response.classification_evaluation_metrics.au_prc], ['AUC ROC', response.classification_evaluation_metrics.au_roc], ['Log Loss', response.classification_evaluation_metrics.log_loss]]   \n",
    "df = pd.DataFrame(data, columns = ['Metric', 'Value']) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Retrieve models and hyperparameter metrics for analysis (approx. 10 minutes)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Retrieving model hyperparameters configuration ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "model_hyperparameters = []\n",
    "parent = automl_client.location_path(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# initialize logger for AutoML Model logs\n",
    "logger = log_client.logger('automl.googleapis.com%2Fmodel')\n",
    "\n",
    "# list TablesModelStructure (e.g. hyperparameters of final model) entries for the specified model\n",
    "filter=f'resource.labels.job_id=\"{MODEL_ID}\" jsonPayload.@type=\"type.googleapis.com/google.cloud.automl.master.TablesModelStructure\"'\n",
    "\n",
    "\n",
    "for entry in logger.list_entries(filter_=filter):\n",
    "    model_parameters = entry.to_api_repr().get(\"jsonPayload\").get(\"modelParameters\")\n",
    "    for model_parameter in model_parameters:\n",
    "      hyperparameters = {}\n",
    "      hyperparameters.update(model_parameter.get(\"hyperparameters\"))\n",
    "      model_hyperparameters.append(hyperparameters)\n",
    "\n",
    "print(\"Display as Pandas dataframe sorted by model type\")\n",
    "print(\"Note: one AutoML model can be a combination of multiple models with different hyperparameters!\")\n",
    "df = pd.DataFrame(model_hyperparameters).sort_values(by=[\"Model type\"],ascending=True)\n",
    "columns = list(df.columns)\n",
    "columns.insert(0, columns.pop(columns.index('Model type')))\n",
    "df = df.reindex(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Retrieve trials tuning metrics for analysis (approx. 10 minutes)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting trials tunning configuration ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "tunning_trials_parameters = []\n",
    "parent = automl_client.location_path(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# initialize logger for AutoML Trials logs\n",
    "logger = log_client.logger('automl.googleapis.com%2Ftuning')\n",
    "\n",
    "# list TablesModelStructure (e.g. hyperparameters of final model) entries for the specified model\n",
    "filter=f'resource.labels.job_id=\"{MODEL_ID}\" jsonPayload.@type=\"type.googleapis.com/google.cloud.automl.master.TuningTrial\"'\n",
    "\n",
    "for entry in logger.list_entries(filter_=filter):\n",
    "    trials_parameters = entry.to_api_repr().get(\"jsonPayload\").get(\"modelStructure\").get(\"modelParameters\")\n",
    "    trainingObjectivePoint = entry.to_api_repr().get(\"jsonPayload\").get(\"trainingObjectivePoint\").get(\"value\")\n",
    "\n",
    "    for trials_parameter in trials_parameters:\n",
    "        hyperparameters = {\"Objective\": trainingObjectivePoint}\n",
    "        hyperparameters.update(trials_parameter.get(\"hyperparameters\"))\n",
    "        tunning_trials_parameters.append(hyperparameters)\n",
    "\n",
    "print(\"Display as Pandas dataframe sorted descending by trainingObjectivePoint\")\n",
    "df = pd.DataFrame(tunning_trials_parameters).sort_values(by=[\"Objective\"],ascending=False)\n",
    "columns = list(df.columns)\n",
    "columns.insert(0, columns.pop(columns.index('Model type')))\n",
    "df = df.reindex(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Export model to GCS (approx. 5 minutes)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Getting model exported at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "model = None\n",
    "try:\n",
    "    GCS_URI=GCS_URI + \"/AUTOML_raw\"\n",
    "    output_config = {}\n",
    "    dataset_full_id = automl_client.model_path(PROJECT_ID, REGION, MODEL_ID)\n",
    "    gcs_destination = automl.types.GcsDestination(output_uri_prefix=GCS_URI)\n",
    "    output_config = automl.types.ModelExportOutputConfig(gcs_destination=gcs_destination, model_format=MODEL_EXPORT)\n",
    "    response = automl_client.export_model(dataset_full_id, output_config)\n",
    "    print(f'Model exported at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n",
    "except:\n",
    "    print(f'Model not ready, try again in few hours, at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
